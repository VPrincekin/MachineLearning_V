专业技能

熟悉 Hadoop 分布式存储原理与工作机制,对分布式有深刻理解,熟悉日常应用场景

熟练使用数据仓库 Hive、HBase 对海量数据进行建模存储

熟悉 Storm,Spark 实时流式处理框架，掌握流式编程。对原理,性能调优有一定的理解

熟悉常见的机器学习算法,基于 Python3 实现过底层源码,机器学习源码重构项目贡献者：https://github.com/apachecn/MachineLearning

熟练使用 SKlearn 机器学习框架,了解 CNN、RNN 以及深度学习框架 Pytorch

熟悉大数据生态系统,了解 Zookeeper,Kafka,Flume 等组件的应用设计及开发


工作经历

2017年8月---2018年3月  百度糯米  大数据开发工程师

主要负责百度糯米数据集市和数据仓库的日常建设&维护,包括：数据模型建设，数据产出，异常问题定位及处理。

2017年10---2018年1月  ApacheCN  机器学习项目模块负责人

1.参加机器学习开源项目。基于《机器学习实战》,实现书中算法并更新迭代至 python3 版本。(目前该项目在 Github 获得2500+ Star)

2.组织参加 Kaggl 机器学习竞赛，梳理一个机器学习项目的完整开发流程。

2016年8月---2017年5月 中数智汇  大数据开发工程师

主要是面向后台数据采集、数据清洗、数据转换、数据稽核等组件开发，用到技术有 Hadoop、、Hive、HBase、Kafka、Spark 等，了解 spring、springmvc、
mybatis 等流行框架。


项目经验

一、百度糯米数据集市平台

项目背景：

    目前我们部门向各业务部门提供数据的渠道主要是灯塔或邮件报表等汇总过后的数据，对于数据量较多的明细数据，大部分是在icafe提单，然后我们来排期实现，这种流程往往周期过长。而且各业务产品对数据提取需求不断增加，需要加大人力对icafe需求单的处理。
    因此，百度糯米数据团队整合糯米各产品线业务数据，从底层 Hive 表中将数据导入数据集市，根据需求建设各类数据集市表，并且支持第三方数据接入，方便各业务团队自助从数据库中快速获取各类汇总，明细，多维数据。

技术要点：

    Hadoop + Hive + Palo + RabbitMQ

主要负责：

    1. 数据集市日常维护&建设：保证集市的正常查询服务，对延迟数据进行处理，异常问题定位。

    2. 数据仓库维护&建设：保证数据的正常产出，对异常任务进行处理，处理新的数据仓库模型建设需求。


二、feed生活服务挂载文章质量判别

项目背景：

    目前糯米的生活服务挂载类文库中有从 feed 导入的部分数据，但 feed 中存在一些消极负面的文章、和生活服务不相关的文章会被推送到生活服务挂载文库。
    此类文章如果直接挂载会对用户体验造成损伤。我们需要对feed推送的文章进行判别，是否应该挂载？

技术要点：

    LDA + SVM

主要负责：

    1. 通过百度提供的 LDA 接口，输入一篇 feed 文章 自动生成该文章的主题分布及其概率。(基于网页的训练，共4798维主题)

    2. 基于 LDA 处理之后的向量，使用 SVM 算法在数据集上训练，交叉验证，超差调优，模型保存。(精确率：92%，召回率：90%)

    (C = 1,kernel="sigmoid",gama = 20,decison_function_shape="ovr")


三、爱购网广告点击流量实时分析系统

项目背景：

    对网站广告点击进行实时监控,并进行统计。可以帮组投资商的投放方向做出正确的判断，同时也有效的降低黑客攻击消耗成本的可能，是保障广告投放于业务顺利开展的必要手段。

技术要点：

    Kafka + SparkCore + SparkSql + SparkStreaming

主要负责：

    1.从Kafka中拉取数据并进行解析。

    2.通过黑名单对数据进行过滤，监控异常点击次数并更新至黑名单。(从Mysql中查询黑名单ID 与获取到的数据 JOIN)

    3.实时统计每天各个城市各品类的广告点击量。



自我评价：

勤于学习，善于总结，乐于分享。能不断提高自身的能力与综合素质。有较强的组织能力与团队精神。
在未来的工作中，我将以充沛的精力，积极的态度努力工作，稳定地提高自己的工作能力，与企业同步发展。